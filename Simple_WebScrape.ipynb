{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, bs4, re\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "import lxml.html\n",
    "\n",
    "# Specify url\n",
    "url = 'https://www.census.gov/programs-surveys/popest.html'\n",
    "\n",
    "\n",
    "# create a BeautifulSoup object from the HTML: soup\n",
    "def make_soup(url):\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.text, 'lxml')\n",
    "    return soup\n",
    "\n",
    "#create a BeautifulSoup object from the HTML: soup\n",
    "soup = make_soup(url)\n",
    "\n",
    "#find all html tags\n",
    "html_tags = soup.find_all('html')\n",
    "\n",
    "\n",
    "#Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all('a') \n",
    "\n",
    "\n",
    "#make a list to append all links into    \n",
    "link_lister = []  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the BeautifulSoup module, we can parse the HTML code and use its various parts.  \n",
    "First we create an object we can use BeautifulSoup against the websites HTML.  \n",
    "Then BeautifulSoup downloads the HTML and parses the tags and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.w3.org/1999/xhtml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/uscensusbureau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.census.gov/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.census.gov/2020census</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.census.gov/2020census</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.census.gov/AmericaCounts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.census.gov/AmericaCounts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.census.gov/AmericaCounts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.census.gov/EconomicCensus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.census.gov/EconomicCensus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0\n",
       "0           http://www.w3.org/1999/xhtml\n",
       "1     https://twitter.com/uscensusbureau\n",
       "2                https://www.census.gov/\n",
       "3      https://www.census.gov/2020census\n",
       "4      https://www.census.gov/2020census\n",
       "5   https://www.census.gov/AmericaCounts\n",
       "6   https://www.census.gov/AmericaCounts\n",
       "7   https://www.census.gov/AmericaCounts\n",
       "8  https://www.census.gov/EconomicCensus\n",
       "9  https://www.census.gov/EconomicCensus"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for a in html_tags:\n",
    "    link = a.get('xmlns')\n",
    "        \n",
    "    if link !=None and link.startswith('http'):\n",
    "        link_lister.append(link)\n",
    "\n",
    "for a in a_tags:\n",
    "    link = a.get('href')\n",
    "        \n",
    "    if link !=None and link.startswith('http'):\n",
    "        link_lister.append(link)\n",
    "\n",
    "#creating a dataframe to organize the data in a more clear way        \n",
    "df = pd.DataFrame(link_lister)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I skimmed the website’s Page Source and looked for links that lead to other sites.  \n",
    "Links always have attributes of ‘href’ so I created a loop that appends all of the href links under the ‘a’ attribute to a list.  \n",
    "The soup.find_all(‘a’) was used to locate all the attributes that would contain ‘href’tags nested like this: \n",
    "\n",
    "    <a href=“https:…>\n",
    "\n",
    "Next I created a for loop, which the a.get(‘href’) would pull the links.  \n",
    "With that loop I used a list, link_lister, to append my websites into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.w3.org/1999/xhtml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/uscensusbureau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.census.gov/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.census.gov/2020census</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.census.gov/2020census</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.census.gov/AmericaCounts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.census.gov/AmericaCounts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.census.gov/AmericaCounts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.census.gov/EconomicCensus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.census.gov/EconomicCensus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0\n",
       "0           http://www.w3.org/1999/xhtml\n",
       "1     https://twitter.com/uscensusbureau\n",
       "2                https://www.census.gov/\n",
       "3      https://www.census.gov/2020census\n",
       "4      https://www.census.gov/2020census\n",
       "5   https://www.census.gov/AmericaCounts\n",
       "6   https://www.census.gov/AmericaCounts\n",
       "7   https://www.census.gov/AmericaCounts\n",
       "8  https://www.census.gov/EconomicCensus\n",
       "9  https://www.census.gov/EconomicCensus"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert /program-survey html's first and append to list\n",
    "\n",
    "#/programs-surveys\n",
    "def get_links(url):\n",
    "    soup = make_soup(url)\n",
    "    a_tags = soup.find_all('a', href=re.compile(r\"^/programs-surveys/\"))\n",
    "    links = [urljoin(url, a['href'])for a in a_tags]  # convert relative url to absolute url\n",
    "    return links\n",
    "\n",
    "new_list = get_links(url)\n",
    "\n",
    "for link in new_list:\n",
    "    link_lister.append(link)        \n",
    "    \n",
    "\n",
    "#/newsroom\n",
    "def get_links(url):\n",
    "    soup = make_soup(url)\n",
    "    a_tags = soup.find_all('a', href=re.compile(r\"^/newsroom/\"))\n",
    "    links = [urljoin(url, a['href'])for a in a_tags]  # convert relative url to absolute url\n",
    "    return links\n",
    "    \n",
    "\n",
    "new_list = get_links(url)\n",
    "\n",
    "for link in new_list:\n",
    "    link_lister.append(link)\n",
    "    \n",
    "\n",
    "#/data/\n",
    "def get_links(url):\n",
    "    soup = make_soup(url)\n",
    "    a_tags = soup.find_all('a', href=re.compile(r\"^/data/\"))\n",
    "    links = [urljoin(url, a['href'])for a in a_tags]  # convert relative url to absolute url\n",
    "    return links\n",
    "    \n",
    "new_list = get_links(url)\n",
    "\n",
    "for link in new_list:\n",
    "    link_lister.append(link)\n",
    "\n",
    "#/library/\n",
    "def get_links(url):\n",
    "    soup = make_soup(url)\n",
    "    a_tags = soup.find_all('a', href=re.compile(r\"^/library/\"))\n",
    "    links = [urljoin(url, a['href'])for a in a_tags]  # convert relative url to absolute url\n",
    "    return links    \n",
    "\n",
    "new_list = get_links(url)\n",
    "\n",
    "for link in new_list:\n",
    "    link_lister.append(link)\n",
    "    \n",
    "df = pd.DataFrame(link_lister)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absolute URL has all the information that points to a resource.  \n",
    "The relative URL will use the absolute URL as a starting to extract the information we want. The relative URI looks at the absolute URL like a pathway.  So, If the absolute URL that is pulled with soup.find_all(‘a’) are like:\n",
    "\t\n",
    "    <a class=\"data-uscb-header-dropdown-link-item uscb-header-dropdown-link-item uscb-padding-TB-10\" href=“https://www.census.gov/about/contact-us/staff-finder.html\">\n",
    "\n",
    "Then the relative URI will look at the pathway like a -> class -> href\n",
    "and that is how we will locate our ‘href’ tags nested inside the ‘a’ attribute.\n",
    "\n",
    "How the scraper works:  \n",
    "    This first portion above only scraps links imbedded in a tags with href attributes that begin with http.  With a for loop it looks at each link identified with an href attribute and appends it to our link_lister list.  Some links have ‘none’ value so those items are ignored with the for loop.\n",
    "    \n",
    "I looked through the Page-Source and identified all of the links that did not have http attached.  The below code shows that I created functions that use the soup object with the url to identify all the a tags by key words: programs-surveys, newsroom, data, and library as they are identified in the leading part of the URI.  Once the link is identified, the leading portion of the URL address is joined with urljoin.  This converts the relative URL to an Absolute URL.\n",
    "\n",
    "After a new list of ‘links’ is created in each function, I created an object new_list and used a for loop to append each item in the new_list to link_lister.  I did this with the four key words that did not have the http portion of the URL shown in the Page Source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.w3.org/1999/xhtml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.w3.org/1999/xhtml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.w3.org/1999/xhtml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.w3.org/1999/xhtml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.w3.org/1999/xhtml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://twitter.com/uscensusbureau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://twitter.com/uscensusbureau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://twitter.com/uscensusbureau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://twitter.com/uscensusbureau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://twitter.com/uscensusbureau</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0\n",
       "0        http://www.w3.org/1999/xhtml\n",
       "1        http://www.w3.org/1999/xhtml\n",
       "2        http://www.w3.org/1999/xhtml\n",
       "3        http://www.w3.org/1999/xhtml\n",
       "4        http://www.w3.org/1999/xhtml\n",
       "5  https://twitter.com/uscensusbureau\n",
       "6  https://twitter.com/uscensusbureau\n",
       "7  https://twitter.com/uscensusbureau\n",
       "8  https://twitter.com/uscensusbureau\n",
       "9  https://twitter.com/uscensusbureau"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_lister.sort()\n",
    "\n",
    "df = pd.DataFrame(link_lister)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above that extracts the unique items from the original link_lister, I’ve created two objects.  \n",
    "One is an empty data set that the duplicates will be added to if duplicates are come across in the for loop used above. and the other is an empty list that unique items will be appended to.  \n",
    "\n",
    "The for loop looks at each item in the link_lister and asks if the item is NOT already in the duplicate data set then it is deemed an original item and thus put inside the unique_items list.  Thus, our duplicates get removed and all unique items will be placed in unique_items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.w3.org/1999/xhtml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/uscensusbureau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.census.gov/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.census.gov/2020census</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.census.gov/AmericaCounts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.census.gov/EconomicCensus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.census.gov/about-us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.census.gov/about/business-opportun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.census.gov/about/contact-us.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.census.gov/about/contact-us/staff-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                       http://www.w3.org/1999/xhtml\n",
       "1                 https://twitter.com/uscensusbureau\n",
       "2                            https://www.census.gov/\n",
       "3                  https://www.census.gov/2020census\n",
       "4               https://www.census.gov/AmericaCounts\n",
       "5              https://www.census.gov/EconomicCensus\n",
       "6                    https://www.census.gov/about-us\n",
       "7  https://www.census.gov/about/business-opportun...\n",
       "8       https://www.census.gov/about/contact-us.html\n",
       "9  https://www.census.gov/about/contact-us/staff-..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing duplicates\n",
    "\n",
    "#create a set that the unique values will go into \n",
    "duplicate = set()\n",
    "#create a list that the duplicate items will go into\n",
    "unique_items = []\n",
    "\n",
    "for x in link_lister:\n",
    "    #if the item is NOT already in the unique_items list then it will be added, else it will be ignored\n",
    "    if x not in duplicate:\n",
    "        unique_items.append(x)\n",
    "        duplicate.add(x)\n",
    "        \n",
    "df = pd.DataFrame(unique_items)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have a unique_items list, I can write the list to a csv file assured that all the links are absolute URLs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "list_len = len(unique_items)\n",
    "\n",
    "#open csv file to write to\n",
    "csv = open('desktop/BEM1 TASK 1/Part 1/link_list.csv','w')\n",
    "\n",
    "for j in range(0,list_len):\n",
    "    csv.write(unique_items[j]+'\\n')#write to each line in csv file\n",
    "    \n",
    "#close the file\n",
    "csv.close()\n",
    "print(csv.closed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code shows the unique HTML list ran including the webpage subject of the extraction.  Underneath it is the portion of the code that shows opening the csv file we will write the list to. Using a for loop to loop through each item in the unique_list, each item gets written to our csv file with the .write function. A sorted list is created with all duplicates removed with the unique_list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
